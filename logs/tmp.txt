Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease
Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Get:4 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1411 kB]
Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Get:6 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [423 kB]
Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2150 kB]
Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [452 kB]
Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2183 kB]
Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]
Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2582 kB]
Fetched 9485 kB in 2s (4355 kB/s)
Reading package lists...
From https://github.com/abhay3010/pytorch-i3d
   1fad907..0a06529  master     -> origin/master
Updating 1fad907..0a06529
Fast-forward
 train_resnet.py | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)
Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
719810 26
488264 26
	 fc.weight
	 fc.bias
Epoch 0/25
  0%|          | 0.00/97.8M [00:00<?, ?B/s] 13%|█▎        | 12.8M/97.8M [00:00<00:00, 134MB/s] 27%|██▋       | 26.5M/97.8M [00:00<00:00, 137MB/s] 41%|████      | 40.0M/97.8M [00:00<00:00, 138MB/s] 57%|█████▋    | 55.9M/97.8M [00:00<00:00, 145MB/s] 77%|███████▋  | 75.1M/97.8M [00:00<00:00, 158MB/s] 98%|█████████▊| 96.0M/97.8M [00:00<00:00, 173MB/s]100%|██████████| 97.8M/97.8M [00:00<00:00, 166MB/s]
step  100 0.16181800067424773
step  200 0.13380081925541162
step  300 0.12250090924402078
step  400 0.11568827468901872
step  500 0.11096535788476466
step  600 0.10735145147889852
step  700 0.10461484260857105
step  800 0.10230228034779429
step  900 0.10033201026419798
step  1000 0.09861712427437305
step  1100 0.09711278214373371
step  1200 0.09579450851927201
step  1300 0.09457362252932329
step  1400 0.09352489744978291
step  1500 0.09253780404229958
step  1600 0.09162393623962999
step  1700 0.09075752485762624
step  1800 0.089998859382338
step  1900 0.08923543556348273
step  2000 0.08854739836230874
step  2100 0.08787177735141345
step  2200 0.08725315777415579
step  2300 0.0866346307746742
step  2400 0.08606629990506917
step  2500 0.08554085143655539
step  2600 0.08503766830007617
step  2700 0.08454808982296122
step  2800 0.08410158797034195
train Loss: 0.0841 
step  100 0.0951465416699648
step  200 0.09472220133990049
step  300 0.0944209282596906
step  400 0.09432666447013617
step  500 0.0944171350300312
step  600 0.09423460585375627
step  700 0.09427984799657549
step  800 0.09425721403211355
step  900 0.0942082464032703
step  1000 0.0941792318969965
step  1100 0.09428624812852253
step  1200 0.09429913380493721
step  1300 0.09429583075527961
step  1400 0.09435363356024026
step  1500 0.09440907427171866
step  1600 0.09443925043568015
step  1700 0.09444252475657884
step  1800 0.09441672038700845
step  1900 0.09438088794287883
val Loss: 0.0944 
Epoch 1/25
step  100 0.07117117069661617
step  200 0.0711629930883646
step  300 0.07097955357283353
step  400 0.07069350559264422
step  500 0.07048496239632368
step  600 0.07025365604708592
step  700 0.07014848620763847
step  800 0.07001022479962557
step  900 0.06992747977789905
step  1000 0.06968932703882456
step  1100 0.06958324553614313
step  1200 0.06949306565336882
step  1300 0.06939774887206462
step  1400 0.06927824990025588
step  1500 0.06912562241156896
step  1600 0.06902741804951802
step  1700 0.06894730835057357
step  1800 0.06881471319124102
step  1900 0.06867094517342354
step  2000 0.06854364259541035
step  2100 0.06843793831410862
step  2200 0.06831114327704364
step  2300 0.06813722392139228
step  2400 0.06801043851921956
step  2500 0.06794555860310793
step  2600 0.06784840838840375
step  2700 0.06774551670584414
step  2800 0.06765626740641892
train Loss: 0.0677 
step  100 0.09543151155114174
step  200 0.09527868587523698
step  300 0.0955796239276727
step  400 0.09574334783479571
step  500 0.09553106199204922
step  600 0.09558565548310677
step  700 0.09563699197556291
step  800 0.09560875163413585
step  900 0.09563169494271279
step  1000 0.09557996454834938
step  1100 0.09549040513282472
step  1200 0.0955689613459011
step  1300 0.09558687936801177
step  1400 0.09558143337390253
step  1500 0.09566024912893772
step  1600 0.09567913164384663
step  1700 0.09562501938027494
step  1800 0.09566367977609237
step  1900 0.09561928485177065
val Loss: 0.0956 
Epoch 2/25
step  100 0.06458440776914358
step  200 0.06463533386588097
step  300 0.06431535165756941
step  400 0.064353123055771
step  500 0.06435185973346233
step  600 0.06437240526080132
step  700 0.06433902393494334
step  800 0.06428213113453239
step  900 0.06415952662212981
step  1000 0.06410736443474889
step  1100 0.06407969929955222
step  1200 0.06402677640629312
step  1300 0.06393361013669234
step  1400 0.06385959287307093
step  1500 0.06384269201258819
step  1600 0.06377827055286617
step  1700 0.06372911838485915
step  1800 0.06369768749094672
step  1900 0.06362947798088978
step  2000 0.06358954625763
step  2100 0.06348887276081812
step  2200 0.06345963636243886
step  2300 0.06342584127803212
step  2400 0.06338388307796171
step  2500 0.06333228393495083
step  2600 0.0633025737808874
step  2700 0.06325069320422631
step  2800 0.06321628777310252
train Loss: 0.0632 
step  100 0.0971240471303463
step  200 0.09718418270349502
step  300 0.09678239139417807
step  400 0.0965865265019238
step  500 0.09670234961807728
step  600 0.09652227286249399
step  700 0.09648840972355434
step  800 0.09663049977272749
step  900 0.09655208239124881
step  1000 0.09650207091867924
step  1100 0.09646897575394674
step  1200 0.09644689849888285
step  1300 0.09645081042670287
step  1400 0.0964673734615956
step  1500 0.0965005494505167
step  1600 0.09648661345243453
step  1700 0.09643269279862152
step  1800 0.09641663755393691
step  1900 0.09638672082439849
val Loss: 0.0964 
Epoch 3/25
step  100 0.061571558155119416
step  200 0.06160121828317642
step  300 0.06173359746734301
step  400 0.06161325213499367
step  500 0.06166496989130974
step  600 0.06157949659973383
step  700 0.061467853555721895
step  800 0.061519350530579685
step  900 0.06149514763719506
step  1000 0.06145063424110413
step  1100 0.06139343959025361
step  1200 0.06132537605861823
step  1300 0.061256404132224045
step  1400 0.06117501703490104
step  1500 0.06117321023096641
step  1600 0.061142364251427356
step  1700 0.06111492563039064
step  1800 0.061089981380436156
step  1900 0.06104298125952482
step  2000 0.06098457278124988
step  2100 0.06095963630115702
step  2200 0.06091007672419602
step  2300 0.06086692481099264
step  2400 0.06085715679917485
step  2500 0.060831535109877585
step  2600 0.06078592920246033
step  2700 0.06074514438846597
step  2800 0.060720007668382356
train Loss: 0.0607 
step  100 0.09697309568524361
step  200 0.09716557376086712
step  300 0.09750211335718632
step  400 0.09757251242175698
step  500 0.09750526770949364
step  600 0.0975996577863892
step  700 0.0974727898729699
step  800 0.09743178537115454
step  900 0.09742464253471958
step  1000 0.09741727930307388
step  1100 0.09733485379002312
step  1200 0.09728886230538289
step  1300 0.09735105838340062
step  1400 0.09730667016335896
step  1500 0.0972776077191035
step  1600 0.09727162987925113
step  1700 0.09734650448841208
step  1800 0.09733560147798724
step  1900 0.09740661920293381
val Loss: 0.0974 
Epoch 4/25
step  100 0.05982262846082449
step  200 0.05968239229172468
step  300 0.05961863808333874
step  400 0.05950252926908434
step  500 0.05959474490582943
step  600 0.05954527223482728
step  700 0.05955685677272933
step  800 0.05955064453650266
step  900 0.05952155169099569
step  1000 0.05950440292060375
step  1100 0.05948350229723887
step  1200 0.059450266227747
step  1300 0.05940612223285895
step  1400 0.05942205119345869
step  1500 0.05940210912376642
step  1600 0.059371467605233194
step  1700 0.05936815373818664
step  1800 0.05931543427829941
step  1900 0.05927422720742853
step  2000 0.059231164935976265
step  2100 0.0592096736867513
step  2200 0.059190347399562596
step  2300 0.0591682710579556
step  2400 0.05915000443036358
step  2500 0.05913309315443039
step  2600 0.05911698066557829
step  2700 0.05909451548010111
step  2800 0.05907265752420894
train Loss: 0.0591 
step  100 0.09891309253871441
step  200 0.09953434154391289
step  300 0.09956536603470643
step  400 0.09921015249565243
step  500 0.09910172356665134
step  600 0.09918779815236728
step  700 0.09906006770474571
step  800 0.09900135221891106
step  900 0.09893689522312747
step  1000 0.09885278517752885
step  1100 0.0988175691799684
step  1200 0.0986716891763111
step  1300 0.09868113784262768
step  1400 0.09864969576575926
step  1500 0.09875832582513491
step  1600 0.09869451252743602
step  1700 0.09862345198059783
step  1800 0.09868635339455473
step  1900 0.09863831932999585
val Loss: 0.0986 
Epoch 5/25
step  100 0.05839712779968977
step  200 0.058013281617313624
step  300 0.058105782431860764
step  400 0.058059156462550165
step  500 0.058089941881597044
step  600 0.058190098001311225
step  700 0.05818546839058399
step  800 0.05825831634458154
step  900 0.05821172410001357
step  1000 0.05817463807761669
step  1100 0.05817211942916567
step  1200 0.058112271695087356
step  1300 0.058122856204326334
step  1400 0.05809633260592818
step  1500 0.058033935606479646
step  1600 0.05801878849975765
step  1700 0.05799393286161563
step  1800 0.0579756884297563
step  1900 0.057943532478652506
step  2000 0.05789303767308593
step  2100 0.057868506409937424
step  2200 0.05784807584333149
step  2300 0.05781903563310271
step  2400 0.05780108589989444
step  2500 0.05778868132680655
step  2600 0.057781434034785396
step  2700 0.05773377247844581
step  2800 0.05770439635030925
train Loss: 0.0577 
